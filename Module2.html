<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Module-2</title>
  </head>
  <body>
      <center><h1><u>Module-2</u></h1></center>
<ol><b>
    <li>

        <h2><u>Rule Of Probability</u></h2>
        <ul><b>
            <li>Product Rule:
            <p>Joint Probability of both a and b; P(A,B)</p>
            <h3>Formula:</h3>
            <center>P(A,B)=P(A|B)*P(B) OR P(A,B)=P(B|A)*P(A)</center>
        </b></li>
         <li><b>Sum Rule:
            <p>If event A is conditionalizes on number of other events B,Then total probabilty of A is sum of its joint probabilities with all B.</p>
            <h3>Formula:</h3>
            <center>P(A,B)=P(A|B)*P(B) OR P(A,B)=P(B|A)*P(A)</center>
        </b></li>
        <li><b>Bayes's Theorem:
            <p>It allows to revers the conditionalizing of events and to compare p(B|A)from knowledge of p(A) and p(B).</p>
            <h3>Formula</h3>
            <center>P(B|A)=(P(A|B)*P(B))/P(A)</center>
        </b></li>
    </ul>
        
    <li><b>
        <h2><u>JOINT ENTROPY:</u></h2>
        <p>The joint entropy H(X,Y) of a pair of discrete random variables (X, Y) with a joint distribution p(x, y) </p>
        <h3>Formula:</h3><center><p>I(X)=-Log2P(x)</p> </center>
        <center><p>H(X,Y)= -Sigmax(Sigmay(P(x,y)*Log(P(x,y))))</p> </center>
        <center><p>H(X,Y)=  Sigmaxy(P(x,y)*Log(1/P(x,y)))</p> </center>
    <center>P:Probabilty</center>
    </b></li>
<li><b><h2><u>MUTUAL INFORMATION:</u></h2></li>
<p>Mutual information between the discrete random variables X and Y is the quantity</p>
<h3>Formula:</h3>
<center>I(X,Y)=H(X)-H(X/Y)</center>
</b></li>
    <li><b>
        <h2><u>KULLBACK-LEIBLER DISTANCE:</u></h2>
       <P>It is also called the information for discrimination.If p(x) and q(x) are two probability distributions defines over the same set of outcomes x.The relative entropy is measure  the inefficiency" of assuming that a distribution is q(x) when in fact it is p(x) </P>
        <h3>Formula:</h3>
        <center><p>Dkl(p||q)=Sigmax(p(x)*Log(P(x)/q(x))</p></center>
    </b></li>
</ol>
  </body>
  </html>